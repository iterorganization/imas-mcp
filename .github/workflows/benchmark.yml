name: Benchmark

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      benchmark_filter:
        description: "Benchmark filter (e.g., SearchBenchmarks)"
        required: false
        default: ""

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  run-benchmark:
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch' || github.event_name == 'pull_request'
    runs-on: ubuntu-22.04 # 4 cores, 16GB RAM, consistent specs
    permissions:
      contents: write  # Need write for gh-pages push

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Need full history for ASV

      - name: Checkout gh-pages branch for benchmark history
        uses: actions/checkout@v4
        with:
          ref: gh-pages
          path: gh-pages-data
        continue-on-error: true  # First run won't have gh-pages yet

      - name: Restore previous benchmark results from gh-pages
        run: |
          if [ -d "gh-pages-data/.asv" ]; then
            echo "Restoring benchmark history from gh-pages..."
            mkdir -p .asv
            cp -r gh-pages-data/.asv/* .asv/ || true
            echo "Restored benchmark history"
            ls -la .asv/results/ || echo "No previous results found"
            # Count previous result files
            PREV_COUNT=$(find .asv/results -name "*.json" 2>/dev/null | wc -l)
            echo "Found $PREV_COUNT previous benchmark result files"
          else
            echo "No previous benchmark history found (this may be the first run)"
          fi

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v2

      - name: Install ASV
        run: uv tool install asv

      - name: Configure ASV for PR context
        if: github.event_name == 'pull_request'
        run: |
          # For PRs, only track the current HEAD to avoid 'main' branch not found errors
          python -c "
          import json
          with open('asv.conf.json', 'r') as f:
              config = json.load(f)
          config['branches'] = ['HEAD']
          with open('asv.conf.json', 'w') as f:
              json.dump(config, f, indent=4)
          print('ASV configured for PR context - tracking only HEAD')
          "

      - name: Configure ASV for main branch
        if: github.event_name != 'pull_request'
        run: |
          # For main branch and other contexts, track main and HEAD
          python -c "
          import json
          with open('asv.conf.json', 'r') as f:
              config = json.load(f)
          config['branches'] = ['main', 'HEAD']
          with open('asv.conf.json', 'w') as f:
              json.dump(config, f, indent=4)
          print('ASV configured for main branch context - tracking main and HEAD')
          "

      - name: Setup ASV machine
        run: |
          # Get actual system specifications from the runner
          CPU_INFO=$(lscpu | grep "Model name" | cut -d: -f2 | sed 's/^[ \t]*//' | head -1)
          CPU_COUNT=$(nproc)
          RAM_MB=$(free -m | grep "Mem:" | awk '{print $2}')
          RAM_GB=$((RAM_MB / 1024))
          ARCH=$(uname -m)

          # Get OS details
          OS_VERSION=$(lsb_release -ds 2>/dev/null | tr -d '"' || echo "Unknown")

          # Create machine name from key specs to detect hardware changes
          # Format: {arch}-{cores}c-{ram}gb
          MACHINE_NAME="${ARCH}-${CPU_COUNT}c-${RAM_GB}gb"
          echo "Machine name: $MACHINE_NAME"

          # Configure machine with detected specs
          asv machine \
            --machine "$MACHINE_NAME" \
            --os "$OS_VERSION" \
            --arch "$ARCH" \
            --cpu "$CPU_INFO" \
            --num_cpu "$CPU_COUNT" \
            --ram "${RAM_GB}GB" \
            --yes

          # Store machine name for benchmark run
          echo "MACHINE_NAME=$MACHINE_NAME" >> $GITHUB_ENV

      - name: Run benchmarks
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          # Run benchmarks
          if [ -n "${{ github.event.inputs.benchmark_filter }}" ]; then
            asv run --python=3.12 --machine "$MACHINE_NAME" -b "${{ github.event.inputs.benchmark_filter }}" --verbose
          else
            asv run --python=3.12 --machine "$MACHINE_NAME" HEAD^! --verbose
          fi

      - name: Show benchmark results on failure
        if: failure()
        run: |
          echo "Benchmark run failed. Checking for logs..."
          find .asv -name "*.log" -exec echo "=== {} ===" \; -exec cat {} \; || true
          echo "Checking ASV results directory..."
          ls -la .asv/ || true
          echo "Checking if schema files exist..."
          ls -la imas_mcp/resources/schemas/ || true

      - name: Generate HTML report
        run: asv publish

      - name: Deploy to GitHub Pages
        if: github.ref == 'refs/heads/main'
        run: |
          # Configure git for pushing
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          
          # Prepare the gh-pages content
          mkdir -p gh-pages-deploy
          
          # Copy HTML report for the website
          cp -r .asv/html/* gh-pages-deploy/
          
          # Copy ASV results and machine config for history persistence
          # This is the critical part - these files accumulate benchmark data over time
          mkdir -p gh-pages-deploy/.asv/results
          cp -r .asv/results/* gh-pages-deploy/.asv/results/ || true
          cp .asv/machine.json gh-pages-deploy/.asv/ || true
          
          # Add .nojekyll to prevent GitHub from processing with Jekyll
          touch gh-pages-deploy/.nojekyll
          
          # Count result files for logging
          RESULT_COUNT=$(find gh-pages-deploy/.asv/results -name "*.json" 2>/dev/null | wc -l)
          echo "Deploying $RESULT_COUNT benchmark result files to gh-pages"
          
          # Navigate to deploy directory and push to gh-pages
          cd gh-pages-deploy
          git init
          git checkout -b gh-pages
          git add -A
          git commit -m "deploy: ${{ github.sha }}"
          
          # Force push to gh-pages branch
          git remote add origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git
          git push -f origin gh-pages
          
          echo "âœ… Deployed to GitHub Pages with $RESULT_COUNT benchmark results preserved"
