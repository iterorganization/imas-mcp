# Strategy Specification: Distributed Semantic Mapping System (Codex & Ambix) 

**System:** Automated IMAS Mapping & Distributed Data Transformation
**Version:** 3.0 (Detailed Technical Baseline)
**Reference:** ITER Organization Open Source Software (`imas-core`, `imas-python`, `imas-data-dictionary`)
**Architecture:** Remote Code Generation / Local Translation (Zero-Install)

---

## 1. Executive Vision

We are implementing a distributed, dual-engine architecture to solve the Semantic Mapping bottleneck in Fusion data. The system decouples **Discovery** (finding the data map) from **Runtime** (using the map).

1. **Project `imas-codex` (The Factory):** An internal service utilizing AI Agents, Remote Code Generation, and Vector Clustering to discover data lineage on remote machines (JET, D3D) and construct a **Federated Knowledge Graph**.
2. **Project `imas-ambix` (The Product):** A public-facing Python client. It executes deterministic "Recipes" to extract raw data using Apache Arrow transport, performing the mapping to **IMAS** objects locally on the user's machine.

We acknowledge that installing and maintaining `imas-python` and `imas-core` on legacy fusion machines is impractical. Therefore, the remote machine acts as a "Dumb Data Pump" (streaming Typed Parquet/Arrow), and the local `imas-ambix` client performs the logical translation into the IDS structure.

---

## 2. Project `imas-codex`: The Builder

**Repository:** `github.com/iter/imas-codex` (Proposed)
**Role:** Discovery & Ontology Management.
**Security Model:** "Code Gen First" (Ephemeral Scripts via SSH).

### 2.1 The Ontology & Data Model
The Graph is the single Source of Truth. It explicitly defines the relationship between the rigid IMAS Data Dictionary (`imas-data-dictionary`) and the chaotic local file systems.

#### 2.1.1 Node Taxonomy (The Nodes)

| Label | Source | Definition | Properties |
| :--- | :--- | :--- | :--- |
| **`DDNode`** | `imas-data-dictionary` | A rigid node defined by the official DD. Defines Structure. | `path` (e.g., `magnetics/flux_loop/flux`), `units`, `dtype` |
| **`Concept`** | `imas-standard-names` | Abstract physics term. | `name` ("Poloidal Flux"), `embedding` (Vector<768>) |
| **`ClusterCentroid`** | Internal ML | Center of a semantic cluster. | `centroid_vector`, `radius` |
| **`LocalVariable`** | Remote Discovery | A distinct signal on a remote machine. | `name` ("MAG_IP_01"), `machine` ("JET"), `native_units` |
| **`DataStore`** | Remote Discovery | The file or logical system holding data. | `type` ("NetCDF", "MDSplus", "HDF5"), `path` |
| **`CodeBlock`** | Remote Discovery | A script or function defining provenance. | `content_hash`, `language` ("Python", "IDL") |

#### 2.1.2 Relationship Taxonomy (The Edges)

**A. Structural (The Spine)**
* `(:DDNode)-[:HAS_CHILD]->(:DDNode)`: Defines the ID tree.
* `(:DDNode)-[:DEPENDS_ON_COORDINATE {axis: int}]->(:DDNode)`: Explicitly links a Signal (Temperature) to its Coordinate (rho_tor_norm). **We do not need separate Dimension nodes; a Coordinate *is* a DDNode acting as a Dimension.**

**B. Semantic (The Discovery)**
* `(:LocalVariable)-[:BELONGS_TO]->(:ClusterCentroid)`: Vector grouping.
* `(:ClusterCentroid)-[:LIKELY_MATCH]->(:Concept)`: AI inference.
* `(:DDNode)-[:MEASURES]->(:Concept)`: Grounding the standard.

**C. Provenance (The Evidence)**
* `(:LocalVariable)-[:DERIVED_FROM]->(:CodeBlock)`: The algorithm.
* `(:CodeBlock)-[:READS_FROM]->(:DataStore)`: The file source.

**D. The Verified Map (The Output)**
* `(:LocalVariable)-[:MAPPED_TO {transform: "x*1e3", scale: 1.0}]->(:DDNode)`: The "Golden Edge" used by Ambix.

---

### 2.2 Ingestion Architecture: The Remote Squad

We use a "Squad" of specialized, ephemeral agents. They are not installed software; they are python scripts generated by LLMs and piped into `python` on the remote host via Fabric.

#### 2.2.1 Agent A: The Cartographer (Scout)
* **Mission:** Map `DataStore` nodes.
* **Technique:** Generates Python wrappers for `find` / `ls -R`.
* **Stopping Strategy (The Boredom Metric):**
    * *Metric:* `NodesCreated / Minute`.
    * *Logic:* "If I scan directory X and 90% of files match existing patterns (e.g., log files, duplicate shots), increment Boredom Counter. If Counter > 3, abort recursion."

#### 2.2.2 Agent B: The Bloodhound (Tracer)
* **Mission:** Trace `CodeBlock` nodes using **`ripgrep`**.
* **Technique:**
    1. Upload `rg` binary to `/tmp/rg`.
    2. Query: "Find where `IP_LA` is assigned."
    3. Generate: `subprocess.run(['/tmp/rg', 'IP_LA\s*=', ...])`.
    4. Parse JSON output -> Graph Node.

#### 2.2.3 Semantic Clustering Strategy
This reduces the mapping search space by 99%.
1. **Ingest:** Scout returns 10,000 variable names/descriptions.
2. **Vectorize:** Codex server computes embeddings (e.g., OpenAI `text-embedding-3-small`).
3. **Cluster:** Apply HDBSCAN or similar to group embeddings into `ClusterCentroid` nodes.
    * *Result:* `MAG_IP`, `IP_LA`, `I_PLASMA` all fall into Cluster #42.
    * *Action:* The LLM only needs to analyze Cluster #42 once to link it to the `Plasma Current` Concept.

---

### 2.3 Security & Access Control

The "Code Gen First" approach requires rigorous boundaries to satisfy Host Facility sysadmins.

* **Identity:** `imas-codex` connects via a service account (e.g., `graph_bot`).
* **Scope:**
    * **READ:** `/public/data`, `/site/codes`.
    * **WRITE:** `/tmp` ONLY.
    * **EXECUTE:** User-space Python, Ripgrep.
* **Network:** No outbound internet from Host. Communication is strictly over the incoming SSH socket (Reverse Tunneling if needed).

---

## 3. Project `imas-ambix`: The Product

**Repository:** `github.com/iter/imas-ambix`
**Role:** Runtime Client & Data Pump.
**Format:** PyPI Package.

### 3.1 The "Recipe" Compilation
Ambix is deterministic. It does not think; it executes.
The "Recipe" is a JSON object compiled from the Codex Graph and stored in **Redis**.

**Recipe Structure:**
```json
{
  "key": "JET/magnetics/ip",
  "data_pump": {
    "driver": "ppf", // Remote driver to use
    "args": ["mag", "ip"] // Arguments for the driver
  },
  "local_transform": {
    "scale": -1.0, 
    "target_node": "magnetics/flux_loop[0]/flux" 
  }
}
```

### 3.2 The Runtime Architecture: "Cloud-Agnostic Pump"

**Phase A: The Pump (Remote)**
1. Ambix (Local) generates a customized `extract.py`.
2. Script is sent via SSH.
3. Script loads native data (e.g., via `MDSplus` or `NetCDF`).
4. Script normalizes data into **Apache Arrow / Parquet**.
5. **Data Egress:** Parquet file is pulled via SFTP (Port 22).
6. *Critically:* No IMAS knowledge is needed here. Just raw arrays + metadata.

**Phase B: The Translation (Local)**
1. Ambix receives the Parquet file.
2. Ambix initializes an empty `imas` object (using `imas-python`).
3. Ambix iterates through the Parquet columns.
4. Ambix applies the `local_transform` (Scaling, Integration).
5. Ambix populates the IDS object.

### 3.3 Handling Complex Data Types (Example: Profiles)

* **Problem:** A Temperature Profile is not just a value; it requires a valid Grid (`rho_tor_norm`).
* **Graph Logic:** The Graph links `Temperature` -> `Grid` via `[:DEPENDS_ON_COORDINATE]`.
* **Ambix Logic:**
    1. Recipe compiler sees the dependency.
    2. Recipe includes *two* fetch instructions: One for `T_e` array, one for `rho` vector.
    3. Data Pump extracts both into a multi-column Parquet file.
    4. Local Transport reads Parquet:
        * `ids.profiles_1d[i].grid.rho_tor_norm = parquet['rho']`
        * `ids.profiles_1d[i].electrons.temperature = parquet['te']`

---

## 4. Implementation Strategy & Roadmap

### Phase 1: Foundations (The Iron Spine)
*Objective: Establish Connectivity and Ontology.*
1. **Repo Init:** `imas-codex` (Docker/Neo4j) and `imas-ambix` (Pip).
2. **Ontology Load:** Write parsers to ingest `imas-data-dictionary` XML into Neo4j `[:DDNode]` structures.
3. **Transport Layer:** Implement `Fabric` implementation in Ambix that supports `put`, `run`, `get` (SFTP).
4. **Proof of Connectivity:** Manually config a recipe for `JET/IP`. Execute Ambix to fetch it to Parquet.

### Phase 2: The Agent Squad (Discovery)
*Objective: Automated Mapping.*
1. **The Scout:** Implement Python-based `find` wrapper with "Boredom Metric" logic.
2. **The Bloodhound:** Implement the `ripgrep` deployer.
    * *Task:* Trace a known variable (`IP_LA`) to its source code file.
3. **Clustering:** Implement the Vector Embedding pipeline to group the 10,000 scanned variables into ~500 concepts.

### Phase 3: The Ambix Runtime (Productization)
*Objective: User-facing Utility.*
1. **Compiler:** Build the script `compile_map.py` that queries Neo4j `[:MAPPED_TO]` edges and populates Redis.
2. **Local Translation:** Implement the `Parquet -> IMAS` mapper using `imas-python`.
3. **Use Case Demo:**
    * Fetch Scalar (Pandas output).
    * Fetch Profile (IDS output).

### Phase 4: Scaling & Refinement
*Objective: Production Readiness.*
1. **Coordinates:** Fully implement the `[:DEPENDS_ON_COORDINATE]` logic for 2D/3D data.
2. **Error Handling:** Robust retry logic for SSH drops during Parquet transfer.
3. **Documentation:** Publish readthedocs for `imas-ambix`.

---

## 5. Technical Evidence: Code Snippets

**Snippet A: Ambix Local Translation (Phase 3)**
```python
import imas
import pyarrow.parquet as pq

def hydrate_ids(parquet_path, recipe):
    # 1. Read the neutral transport format
    table = pq.read_table(parquet_path)
    
    # 2. Init the strict IMAS object (Local Only)
    ids = imas.ids(recipe['target_ids'])
    
    # 3. Map columns to Schema
    for col_name in table.column_names:
        target_path = recipe['mappings'][col_name]['target_path']
        data = table[col_name].to_numpy()
        
        # Apply scaling defined in Recipe
        scale = recipe['mappings'][col_name].get('scale', 1.0)
        
        # Set data (Pseudocode for OMAS/IMAS setter)
        ids.set_data(target_path, data * scale)
        
    return ids
```

**Snippet B: Codex Boredom Metric (Phase 2)**
```python
def scan_directory(agent, path):
    known_signatures = agent.load_signatures() # Bloom Filter of known files
    new_info_count = 0
    
    for file in agent.ls(path):
        if file.hash not in known_signatures:
            new_info_count += 1
            agent.graph.create_node(file)
            
    # The Metric
    if new_info_count < 5: 
        agent.boredom += 1
    else:
        agent.boredom = 0
        
    if agent.boredom > 3:
        raise StopIteration("Directory Saturated - Moving on")
```

---

