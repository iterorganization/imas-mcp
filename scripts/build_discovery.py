#!/usr/bin/env python3
"""
Build discovery models and schema documentation from LinkML schemas.

This module generates:
1. Pydantic models from each LinkML schema file in ontology/discovery/
2. Markdown documentation for prompts from each schema file

The generated files should NOT be edited directly - edit the LinkML schemas instead.
"""

import logging
import subprocess
import sys
from pathlib import Path

import click
from linkml_runtime.utils.schemaview import SchemaView

logger = logging.getLogger(__name__)


def get_project_root() -> Path:
    """Get the project root directory."""
    return Path(__file__).parent.parent


def get_discovery_schema_dir() -> Path:
    """Get the directory containing discovery LinkML schemas."""
    return get_project_root() / "imas_codex" / "ontology" / "discovery"


def get_models_output_dir() -> Path:
    """Get the directory for generated Pydantic models."""
    return get_project_root() / "imas_codex" / "discovery" / "models"


def get_schema_docs_dir() -> Path:
    """Get the directory for generated schema documentation."""
    return get_project_root() / "imas_codex" / "prompts" / "agents" / "schemas"


def needs_rebuild(source: Path, target: Path) -> bool:
    """Check if target needs to be rebuilt based on source modification time."""
    if not target.exists():
        return True
    return source.stat().st_mtime > target.stat().st_mtime


def generate_pydantic_model(schema_file: Path, output_file: Path) -> bool:
    """Generate Pydantic model from a LinkML schema file.

    Args:
        schema_file: Path to the LinkML YAML schema
        output_file: Path to write the generated Python file

    Returns:
        True if generation succeeded, False otherwise
    """
    cmd = ["gen-pydantic", "--black", str(schema_file)]

    result = subprocess.run(cmd, capture_output=True, text=True, check=False)

    if result.returncode != 0:
        logger.error(f"gen-pydantic failed for {schema_file}: {result.stderr}")
        return False

    # Add header comment
    header = f'''"""
Discovery Models - {schema_file.stem}

AUTO-GENERATED from {schema_file.relative_to(get_project_root())}
DO NOT EDIT THIS FILE DIRECTLY - edit the LinkML schema instead.

To regenerate:
    uv run build-discovery --force
"""

'''
    generated_code = header + result.stdout

    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text(generated_code)
    logger.info(f"Generated model: {output_file}")
    return True


def generate_schema_docs(schema_file: Path, output_file: Path) -> bool:
    """Generate Markdown documentation from a LinkML schema file.

    Uses SchemaView to extract class information and generate
    documentation suitable for inclusion in exploration prompts.

    Args:
        schema_file: Path to the LinkML YAML schema
        output_file: Path to write the generated Markdown file

    Returns:
        True if generation succeeded, False otherwise
    """
    try:
        sv = SchemaView(str(schema_file))
    except Exception as e:
        logger.error(f"Failed to load schema {schema_file}: {e}")
        return False

    lines = [
        f"<!-- AUTO-GENERATED from {schema_file.name} - DO NOT EDIT -->",
        "",
        "## Expected Output Structure",
        "",
        f"When finishing {schema_file.stem} exploration, provide YAML matching this structure:",
        "",
    ]

    # Get all classes defined in this schema
    for class_name in sv.all_classes():
        cls = sv.get_class(class_name)
        if cls is None:
            continue

        # Skip imported/abstract classes for the main documentation
        if cls.abstract:
            continue

        lines.append(f"### {class_name}")
        lines.append("")

        if cls.description:
            lines.append(f"{cls.description}")
            lines.append("")

        # Build attribute table
        attrs = sv.class_induced_slots(class_name)
        if attrs:
            lines.append("| Field | Type | Required | Description |")
            lines.append("|-------|------|----------|-------------|")

            for slot in attrs:
                # class_induced_slots returns SlotDefinition objects
                attr_name = slot.name

                # Determine type
                range_type = slot.range or "string"
                if slot.multivalued:
                    range_type = f"list[{range_type}]"

                # Determine if required
                required = "yes" if slot.required else "no"

                # Get description
                description = slot.description or ""

                lines.append(
                    f"| `{attr_name}` | {range_type} | {required} | {description} |"
                )

            lines.append("")

    # Generate example YAML structure
    lines.append("### Example")
    lines.append("")
    lines.append("```yaml")

    # Generate example for the main artifact class
    for class_name in sv.all_classes():
        cls = sv.get_class(class_name)
        if cls is None or cls.abstract:
            continue
        if "Artifact" in class_name:
            lines.extend(_generate_example_yaml(sv, class_name, indent=0))
            break

    lines.append("```")
    lines.append("")

    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text("\n".join(lines))
    logger.info(f"Generated schema docs: {output_file}")
    return True


def _generate_example_yaml(
    sv: SchemaView,
    class_name: str,
    indent: int = 0,
    visited: set[str] | None = None,
) -> list[str]:
    """Generate example YAML for a class.

    Args:
        sv: SchemaView instance
        class_name: Name of the class to generate example for
        indent: Current indentation level
        visited: Set of already visited class names (for recursion protection)
    """
    if visited is None:
        visited = set()

    # Recursion protection for self-referencing types
    if class_name in visited:
        return [f"{'  ' * indent}# ... (recursive reference to {class_name})"]

    visited = visited | {class_name}  # Create new set to not affect siblings

    lines = []
    prefix = "  " * indent

    for slot in sv.class_induced_slots(class_name):
        # class_induced_slots returns SlotDefinition objects
        attr_name = slot.name

        # Skip facility and explored_at - added automatically
        if attr_name in ("facility", "explored_at", "explorer_model"):
            continue

        range_type = slot.range or "string"

        # Generate example value based on type
        if slot.multivalued:
            lines.append(f"{prefix}{attr_name}:")
            if range_type in ("string", "str"):
                lines.append(f'{prefix}  - "example_value"')
            elif sv.get_class(range_type):
                # Nested class
                nested_lines = _generate_example_yaml(
                    sv, range_type, indent + 1, visited
                )
                if nested_lines:
                    lines.append(f"{prefix}  -")
                    lines.extend([f"  {line}" for line in nested_lines])
            else:
                lines.append(f"{prefix}  - example")
        elif sv.get_class(range_type):
            # Nested object
            lines.append(f"{prefix}{attr_name}:")
            lines.extend(_generate_example_yaml(sv, range_type, indent + 1, visited))
        elif range_type == "boolean":
            lines.append(f"{prefix}{attr_name}: true")
        elif range_type in ("integer", "int"):
            lines.append(f"{prefix}{attr_name}: 0")
        elif range_type == "datetime":
            lines.append(f'{prefix}{attr_name}: "2025-01-01T00:00:00Z"')
        else:
            lines.append(f'{prefix}{attr_name}: "example_value"')

    return lines


def build_all(force: bool = False, dry_run: bool = False) -> tuple[int, int]:
    """Build all discovery models and schema documentation.

    Args:
        force: Force rebuild even if files are up to date
        dry_run: Show what would be generated without writing

    Returns:
        Tuple of (models_generated, docs_generated)
    """
    schema_dir = get_discovery_schema_dir()
    models_dir = get_models_output_dir()
    docs_dir = get_schema_docs_dir()

    if not schema_dir.exists():
        logger.warning(f"Schema directory not found: {schema_dir}")
        return 0, 0

    models_generated = 0
    docs_generated = 0

    # Process each schema file (skip files starting with _)
    for schema_file in sorted(schema_dir.glob("*.yaml")):
        if schema_file.name.startswith("_"):
            logger.debug(f"Skipping base schema: {schema_file.name}")
            continue

        stem = schema_file.stem

        # Generate Pydantic model
        model_file = models_dir / f"{stem}.py"
        if force or needs_rebuild(schema_file, model_file):
            if dry_run:
                click.echo(f"Would generate model: {model_file}")
            else:
                if generate_pydantic_model(schema_file, model_file):
                    models_generated += 1

        # Generate schema documentation
        docs_file = docs_dir / f"{stem}_schema.md"
        if force or needs_rebuild(schema_file, docs_file):
            if dry_run:
                click.echo(f"Would generate docs: {docs_file}")
            else:
                if generate_schema_docs(schema_file, docs_file):
                    docs_generated += 1

    # Generate __init__.py for models package
    if not dry_run and models_generated > 0:
        _generate_models_init(models_dir, schema_dir)

    return models_generated, docs_generated


def _generate_models_init(models_dir: Path, schema_dir: Path) -> None:
    """Generate __init__.py that re-exports all models."""
    imports = []
    exports = []

    for schema_file in sorted(schema_dir.glob("*.yaml")):
        if schema_file.name.startswith("_"):
            continue
        stem = schema_file.stem
        imports.append(f"from .{stem} import *  # noqa: F401, F403")
        exports.append(stem)

    content = '''"""
Discovery Models Package.

AUTO-GENERATED - DO NOT EDIT.
Re-exports all generated models from discovery schemas.
"""

'''
    content += "\n".join(imports) + "\n"

    init_file = models_dir / "__init__.py"
    init_file.write_text(content)
    logger.info(f"Generated: {init_file}")


@click.command()
@click.option("--verbose", "-v", is_flag=True, help="Enable verbose logging")
@click.option("--quiet", "-q", is_flag=True, help="Suppress all logging except errors")
@click.option("--force", "-f", is_flag=True, help="Force rebuild even if up to date")
@click.option("--dry-run", is_flag=True, help="Show what would be generated")
def build_discovery(verbose: bool, quiet: bool, force: bool, dry_run: bool) -> int:
    """Generate Pydantic models and schema docs from LinkML discovery schemas.

    This command processes all YAML files in ontology/discovery/ and generates:
    - Pydantic models in discovery/models/
    - Schema documentation in prompts/agents/schemas/

    Examples:
        build-discovery              # Generate models and docs
        build-discovery -v           # Verbose output
        build-discovery --dry-run    # Preview without writing
        build-discovery -f           # Force regeneration
    """
    # Set up logging
    if quiet:
        log_level = logging.ERROR
    elif verbose:
        log_level = logging.DEBUG
    else:
        log_level = logging.INFO

    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    try:
        models, docs = build_all(force=force, dry_run=dry_run)

        if dry_run:
            click.echo("Dry run complete.")
        else:
            click.echo(f"Generated {models} models and {docs} schema docs.")

        return 0

    except Exception as e:
        logger.error(f"Build failed: {e}")
        if verbose:
            logger.exception("Full traceback:")
        click.echo(f"Error: {e}", err=True)
        return 1


if __name__ == "__main__":
    sys.exit(build_discovery())
